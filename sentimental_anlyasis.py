# -*- coding: utf-8 -*-
"""Sentimental Anlyasis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DhQyYUk9Zq6FRlPRqekNds7ge8ooHABP
"""

import pandas as pd
import tensorflow_datasets as tfds

dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
train_ds, test_ds = dataset['train'], dataset['test']

# Convert to pandas DataFrame (example: first 5 entries)
import itertools
examples = list(itertools.islice(train_ds, 5))
df = pd.DataFrame({
    'review': [str(example[0].numpy().decode('utf-8')) for example in examples],
    'label': [int(example[1].numpy()) for example in examples]
})
print(df)

from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Load data (top 10k words)
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)

# Decode word indices to text (requires loading word index)
from tensorflow.keras.datasets import imdb
word_index = imdb.get_word_index()
index_rev = {i+3: word for word, i in word_index.items()}
index_rev[0], index_rev[1], index_rev[2] = '[pad]', '[start]', '[unk]'

def decode_review(encoded):
    return ' '.join(index_rev.get(i, '?') for i in encoded)

print("Sample review:", decode_review(X_train[0]))
print("Label:", y_train[0])

# Step 1: Download dataset from Stanford
!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz

# Step 2: Extract the dataset
!tar -xvzf aclImdb_v1.tar.gz

import os
import pandas as pd

def load_imdb_data(base_path='aclImdb', subset='train'):
    data = {'review': [], 'sentiment': []}
    for label in ['pos', 'neg']:
        folder = os.path.join(base_path, subset, label)
        for fname in os.listdir(folder):
            with open(os.path.join(folder, fname), 'r', encoding='utf-8') as f:
                data['review'].append(f.read())
                data['sentiment'].append(label)
    return pd.DataFrame(data)

df_train = load_imdb_data(subset='train')
df_test  = load_imdb_data(subset='test')

print("Train shape:", df_train.shape)
print("Test shape:", df_test.shape)

# Sentiment Analysis Project - CodeAlpha Task 3 (All-in-One Cell)

# Step 1: Install + Import libraries
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
nltk.download('stopwords')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Step 2: Download and Extract IMDB Dataset
!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xvzf aclImdb_v1.tar.gz

# Step 3: Load dataset into Pandas DataFrame
def load_imdb_data(base_path='aclImdb', subset='train'):
    data = {'review': [], 'sentiment': []}
    for label in ['pos', 'neg']:
        folder = os.path.join(base_path, subset, label)
        for fname in os.listdir(folder):
            with open(os.path.join(folder, fname), 'r', encoding='utf-8') as f:
                data['review'].append(f.read())
                data['sentiment'].append(label)
    return pd.DataFrame(data)

df_train = load_imdb_data(subset='train')
df_test  = load_imdb_data(subset='test')

# Step 4: Clean Text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'<.*?>', '', text)              # remove HTML
    text = re.sub(r'[^a-zA-Z]', ' ', text)         # keep only letters
    text = re.sub(r'\s+', ' ', text)               # remove extra spaces
    return text

df_train['review'] = df_train['review'].apply(clean_text)
df_test['review']  = df_test['review'].apply(clean_text)

# Step 5: Features + Labels
X_train = df_train['review']
y_train = df_train['sentiment'].map({'pos':1, 'neg':0})
X_test  = df_test['review']
y_test  = df_test['sentiment'].map({'pos':1, 'neg':0})

# Step 6: TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec  = vectorizer.transform(X_test)

# Step 7: Train Logistic Regression
model = LogisticRegression(max_iter=200)
model.fit(X_train_vec, y_train)

# Step 8: Evaluate
y_pred = model.predict(X_test_vec)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative','Positive'],
            yticklabels=['Negative','Positive'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Step 9: Custom Prediction
sample_review = "The movie was absolutely fantastic! I loved it."
sample_vec = vectorizer.transform([sample_review])
prediction = model.predict(sample_vec)[0]
print("\n Sample Review:", sample_review)
print("Predicted Sentiment:", "Positive" if prediction==1 else "Negative")